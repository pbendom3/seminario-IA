[[./imagenes/seminario21.PNG]]

* √âtica en el uso de la IA.
[[./imagenes/ia_etica.png]]

** ‚ö†Ô∏è Novedad!!: ALIA, la inteligencia artificial espa√±ola. [[https://alia.gob.es/][Acceso a ALIA]]


** √çndice
    1. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#1-implicaciones-%C3%A9ticas-y-sociales-de-los-sistemas-de-ia][Implicaciones √©ticas y sociales de los sistemas de IA.]]
    2. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#2-%C3%A9tica-en-el-aula][√âtica en el aula.]]
    3. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#2-sora-ia-el-motor-de-inteligencia-artificial-capaz-de-generar-v%C3%ADdeo-realista][Sora IA, el motor de inteligencia artificial capaz de generar v√≠deo realista.]]
    4. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#3-herramientas-asistidas-por-ia-que-se-utilizan-para-crear-y-distribuir-informaci%C3%B3n--tanto-informaci%C3%B3n-objetiva-como-desinformaci%C3%B3n-bulos-][Herramientas asistidas por IA que se utilizan para crear y distribuir informaci√≥n -tanto informaci√≥n objetiva como desinformaci√≥n (bulos)-.]]
    5. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#4-introducci%C3%B3n-al-sesgo-con-moralmachine][Introducci√≥n al sesgo con moralmachine.]] 
    6. [[https://github.com/pbendom3/seminario-IA/blob/main/sesion-2.org#5-documental-sesgo-codificado-2020-netflix][Documental: Sesgo Codificado (2020, NETFLIX).]] 
   
** Referencias
- [[https://formacion.intef.es/aulaenabierto/mod/book/view.php?id=5073][INTEF - √âtica en la inteligencia artificial]]
- [[https://www.youtube.com/watch?v=SPBn9gwgIsI&t=95s][V√≠deo sobre Sora IA]] 
- [[https://www.moralmachine.net/hl/es][Acceso a Moral Machine]]
- [[https://www.netflix.com/es/title/81328723][Documental: Sesgo Codificado (2020, NETFLIX)]] 
- [[https://maldita.es/malditatecnologia/20241230/uso-2024-inteligencia-artificial-bulos-desinformar/][C√≥mo se ha usado en 2024 la inteligencia artificial para difundir bulos y desinformar]]
-  [[https://www.linkedin.com/pulse/cr%C3%ADticos-digitales-versus-vagos-d%C3%B3nde-te-posicionas-t%C3%BA-%C3%A0ngels-soriano-bo0tf/][‚ÄúCr√≠ticos Digitales Versus Vagos digitales‚Äù ¬øD√≥nde te posicionas T√∫?]]
- [[https://unesdoc.unesco.org/ark:/48223/pf0000391105][Documento de La UNESCO sobre el Marco de referencia de las competencias en IA para los estudiantes]]
- [[https://artificialintelligenceact.eu/es/article/5/][UE, Pr√°cticas prohibidas de IA]]


** 1. Implicaciones √©ticas y sociales de los sistemas de IA

Como culturilla general, cuando hablamos de la √©tica en las Inteligencias Artificiales no solamente debemos referirnos al modo en que utilizan los datos, si no tambi√©n en la forma de obtenerlos y de guardarlos.

[[./imagenes/ethics.png]]

En general, los 3 ejes que se deben respetar siempre son:

- Consentimiento y privacidad.
- Seguridad y protecci√≥n de los datos.
- No discriminaci√≥n.

La suma de estos conceptos implica garantizar que la informaci√≥n personal se recopile y maneje de manera que se protejan los derechos de las personas a la privacidad. Esto incluye obtener el consentimiento informado para la recopilaci√≥n de datos o implementar medidas para proteger los datos personales. 

Adem√°s, se deben salvaguardar los datos contra el acceso no autorizado, las violaciones de seguridad y otras formas de uso indebido.

Por √∫ltimo, debemos garantizar que los datos se recopilen, procesen y utilicen de manera justa y no conduzca a ning√∫n tipo de discriminaci√≥n. Implica evitar sesgos en la recopilaci√≥n y el an√°lisis de datos, garantizar que los algoritmos y las decisiones basadas en datos no perpet√∫en la desigualdad y contribuir a lograr la inclusi√≥n.

~Adem√°s de esas 3 premisas, tambi√©n deber√≠a cumplirse el principio de Transparencia: ¬øSabemos c√≥mo se toman las decisiones algor√≠tmicas? Debatimos con el caso de hashtag#Tiktok~

~Por otro lado, deber√≠amos tener cierta Responsabilidad: ¬øQui√©n debe rendir cuentas cuando una IA comete un error? ¬øQui√©n es el responsable final, nosotros los que escribimos los prompts o la IA que ejecuta el da√±o?~

üåü La √©tica no es s√≥lo un marco de referencia, sino una responsabilidad compartida entre quienes desarrollamos, implementamos y utilizamos la IA. 

üåü *Meritxell Bretos: "La tecnolog√≠a es una herramienta poderosa, pero su impacto depender√° siempre de las decisiones humanas detr√°s de ella".*

üåü Y por √∫ltimo, un [[https://www.linkedin.com/posts/ramon-lopez-roldan-724b2686_ia-dalladbe-ethics-activity-7279767543030927361-zdjJ/?utm_source=share&utm_medium=member_ios][post de Ramon Lopez Roldan]] donde se sorprende de ver el protagonismo que le ha dado DALL-E a la palabra hashtag#ethics, en el 2024, pidi√©ndole que le generara una imagen con el resumen de 2024 y la importancia que la IA ha tenido en √©l.

[[./imagenes/ethics2.jpg]]


** 2. √âtica en el aula

¬øSi detect√°is que un trabajo es copiado por parte de alg√∫n estudiante c√≥mo lo evalu√°is? ¬øLo hab√©is contemplado en el apartado de Evaluaci√≥n de vuestra Programaci√≥n Did√°ctica?

*** Cr√≠ticos Digitales vs Vagos digitales



** 3. Sora IA, el motor de inteligencia artificial capaz de generar v√≠deo realista

[[https://sora.com/][Sora]] es el modelo de IA generativa de texto a v√≠deo de OpenAI. Esto significa que t√∫ escribes un texto y √©l crea un v√≠deo que coincide con la descripci√≥n del texto. Aqu√≠ tienes unos ejemplos:

[[https://www.youtube.com/watch?v=SPBn9gwgIsI&t=95s][./imagenes/sora.PNG]] 

Sora est√° disponible en la mayor parte del mundo, excepto en la mayor√≠a de pa√≠ses de Europa y el Reino Unido. En Espa√±a todav√≠a no tenemos acceso, pero como alternativa podemos probar:

- Pika
- Runway -- https://runwayml.com/ 

[[./gif/erasmus.mp4][./gif/erasmus.jpeg]] 

[[./gif/erasmus.mp4]]


*** Los riesgos de Sora

- *Generaci√≥n de contenidos nocivos*

Sin barreras de protecci√≥n, Sora puede generar contenidos desagradables o inapropiados, como v√≠deos con violencia, gore, material sexual expl√≠cito, representaciones despectivas de grupos de personas y otras im√°genes de odio, as√≠ como la promoci√≥n o glorificaci√≥n de actividades ilegales.

Lo que constituye contenido inapropiado var√≠a mucho en funci√≥n del usuario (piensa en un ni√±o que utiliza Sora frente a un adulto) y del contexto de la generaci√≥n del v√≠deo (un v√≠deo que advierte sobre los peligros de los fuegos artificiales podr√≠a convertirse f√°cilmente en sangriento de forma educativa).

- *Desinformaci√≥n*

Seg√∫n los v√≠deos de ejemplo compartidos por OpenAI, uno de los puntos fuertes de Sora es su capacidad para crear escenas fant√°sticas que no podr√≠an existir en la vida real. Esta fuerza tambi√©n hace posible crear v√≠deos "deepfake" en los que personas o situaciones reales se transforman en algo que no es verdad.

Cuando este contenido se presenta como verdad, ya sea accidentalmente (desinformaci√≥n) o deliberadamente (desinformaci√≥n), puede causar problemas.

Como escribi√≥ [[https://www.linkedin.com/pulse/navigating-ai-impact-elections-2024-digidiplomacy-icdhe/][Eske Montoya Mart√≠nez van Egerschot, Jefa de Gobernanza y √âtica de la IA en DigiDiplomacy]], "la IA est√° remodelando las estrategias de campa√±a, la participaci√≥n de los votantes y el propio tejido de la integridad electoral".

Los v√≠deos de IA convincentes pero falsos de pol√≠ticos o adversarios de pol√≠ticos tienen el poder de "difundir estrat√©gicamente narrativas falsas y acosar a fuentes leg√≠timas, con el objetivo de socavar la confianza en las instituciones p√∫blicas y fomentar la animadversi√≥n hacia diversas naciones y grupos de personas".

En un a√±o con muchas elecciones importantes, desde Taiw√°n hasta la India y Estados Unidos, esto tiene amplias consecuencias.

- *Prejuicios y estereotipos*

Como ya hemos venido comentando durante las sesiones, el resultado de los modelos generativos de IA depende en gran medida de los datos con los que se han entrenado. Eso significa que los sesgos o estereotipos culturales en los datos de entrenamiento pueden provocar los mismos problemas en los v√≠deos resultantes. Como [[https://www.datacamp.com/es/podcast/fighting-for-algorithmic-justice-with-dr-joy-buolamwini-artist-in-chief-and-president-of-the-algorithmic-justice-league][Joy Buolamwini expuso en el episodio Luchando por la Justicia Algor√≠tmica de DataFramed]], los sesgos en las im√°genes pueden tener graves consecuencias en la actuaci√≥n policial.


** 4. Herramientas asistidas por IA que se utilizan para crear y distribuir informaci√≥n -tanto informaci√≥n objetiva como desinformaci√≥n (bulos)-


** 5. Introducci√≥n al sesgo con moralmachine

Dir√≠gete al siguiente sitio web, donde vas a entrenar a un coche autom√°tico para tomar una serie de decisiones: https://www.moralmachine.net/

Es un ¬´juego¬ª en l√≠nea multiling√ºe que se plante√≥ para recoger datos sobre c√≥mo querr√≠an los ciudadanos que los veh√≠culos aut√≥nomos resolvieran dilemas morales en el contexto de accidentes inevitables, es decir, para evaluar las expectativas sociales sobre la manera en que los veh√≠culos aut√≥nomos tendr√≠an que resolver dilemas morales.

[[./imagenes/moral.PNG]]

‚ö†Ô∏è *AVISO*: lamentablemente, algunas decisiones ser√°n poco √©ticas...üò•‚ö†Ô∏è

Analizando las m√°s de un mill√≥n de respuestas, se ve que hay diferencias culturales respecto a las preferencias para cada escenario. Algunas culturas prefieren salvar a los j√≥venes; otras, a las mujeres, y otras, a las personas de alto estatus.

En cualquier caso, si el sistema del veh√≠culo aut√≥nomo tiene acceso a informaci√≥n personal, cualquier decisi√≥n no aleatoria entre dos vidas siempre ser√° discriminatoria. Si la decisi√≥n es aleatoria, tiene que considerar todas las posibilidades por igual y, por lo tanto, tambi√©n tendr√° que incluir al conductor del coche. Quiz√°s este hecho no va muy a favor de las ventas de veh√≠culos aut√≥nomos. Por otro lado, si las vidas de los conductores de coches aut√≥nomos se priorizan siempre por encima de las de los peatones, posiblemente la gente que ¬´corra el riesgo¬ª de ser peat√≥n ser√° la que no se pueda permitir tener un veh√≠culo aut√≥nomo.

** 6. Documental: Sesgo Codificado (2020, NETFLIX)

[[https://www.netflix.com/es/title/81328723][Este documental]] investiga errores en los algoritmos despu√©s de que Joy Buolamwini, investigadora del MIT, revelara fallos en la tecnolog√≠a de reconocimiento facial.

[[./imagenes/sesgo2.jpeg]]

En concreto, Joy se da cuenta de que un programa de reconocimiento facial no distingue ni identifica su rostro como el de una persona cuantificable para su base de datos. Pero s√≠ lo hace cuando se coloca una m√°scara neutra... y blanca.

Es el punto de partida de un documental que picotea en much√≠simos temas, todos a partir de la arbitrariedad y falta de √©tica con la que los algoritmos recogen informaci√≥n para dar forma a sus bases de datos y los conocimientos con los que van engordando distintas IAs. Una arbitrariedad que toma forma a partir de prejuicios que todos tenemos y que hacen que, por ejemplo, y como dice uno de los participantes en este interesante documental, *el racismo se mecanice y se replique*.

*** ¬øC√≥mo podemos protegernos contra el sesgo y la discriminaci√≥n cuando los conjuntos de datos de formaci√≥n pueden prestarse al sesgo?

Aunque las empresas suelen tener buenas intenciones en torno a sus esfuerzos de automatizaci√≥n, la incorporaci√≥n de la IA en, por ejemplo, las pr√°cticas de contrataci√≥n, puede tener consecuencias imprevistas. En su esfuerzo por automatizar y simplificar un proceso, Amazon, "sin querer", sesg√≥ por g√©nero a los posibles candidatos laborales [[https://incidentdatabase.ai/cite/37/][(enlace externo a ibm.com)]] para los puestos t√©cnicos abiertos y, en √∫ltima instancia, tuvo que descartar el proyecto.

A medida que las empresas son m√°s conscientes de los riesgos de la IA, tambi√©n se han vuelto m√°s activas en este debate sobre la √©tica y los valores de la IA. Por ejemplo, el a√±o pasado el CEO de IBM, Arvind Krishna, comparti√≥ que IBM ha puesto fin a sus productos de reconocimiento y an√°lisis facial para uso general, haciendo hincapi√© en que "IBM se opone firmemente y no aprobar√° los usos de ninguna tecnolog√≠a, incluida la tecnolog√≠a de reconocimiento facial ofrecida por otros proveedores, para la vigilancia masiva, la elaboraci√≥n de perfiles raciales, las violaciones de los derechos humanos y las libertades b√°sicas, o cualquier prop√≥sito que no sea coherente con nuestros valores y Principios de Confianza y Transparencia".





